### This repo describes the experimental details on Wikitext-103 benchmark.
#### 1. Data Preparation:
To download the data, please follow the instructions [here](https://github.com/yxuansu/SimCTG/tree/main/data).

> **** The dataset contains the following three files:

    .
    ├── wikitext103                       
        ├── wikitext103_raw_v1_train.txt          # Training Set
        ├── wikitext103_raw_v1_validation.txt     # Validation Set
        └── wikitext103_raw_v1_test.txt           # Test Set

**Data Format**: In the files, each line represents a full document.


#### 2. Train SimCTG:

#### 3. Generate Results:
Here, we use the prefix in Table 3 of the [paper]() to illustrate how to use different decoding methods to generate the result. 
```python
import torch
from simctg import SimCTG
from transformers import AutoTokenizer
# load model and tokenizer
model_path = r'cambridgeltl/simctg_wikitext103'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = SimCTG(model_path, tokenizer.pad_token_id)
model.eval()

# prepare prefix input
text = r"Butt criticized Donald 's controls in certain situations in the game , as well as the difficulty of some levels and puzzles . Buchanan also criticized the controls , calling"
tokens = tokenizer.tokenize(text)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.LongTensor(input_ids).view(1,-1)
```
##### (1) Contrastive Search:
```python
# use contrastive search to generate the result
beam_width, alpha, decoding_len = 8, 0.6, 128
output = model.fast_contrastive_search(input_ids, beam_width, alpha, decoding_len)
#output = model.slow_contrastive_search(input_ids, beam_width, alpha, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--beam_width`: k in the contrastive search, which is typically set within the range of [0.5,0.8].
* `--alpha`: alpha in the contrastive search, which is typically set within the range of [5,10].
* `--decoding_len`: Number of tokens to generate.

**[Note]** We provide two implementations of contrastive search: (1) fast_contrastive_search and (2) slow_contrastive_search. These two implementations produce the same result, but the fast version is properly optimized and is much faster than the slow version. On the other hand, the implementation details of the slow version is more straightforward. We recommend you to rewrite the slow version first if you want to adapt contrastive search to your specific research task.

##### (2) Diverse Contrastive Search:
We also provide a stochastic version of contrastive search which can generate diverse results by combining nucleus sampling and contrastive search. More details can be found in Appendix E of the [paper]().
```python
# use diverse contrastive search to generate the result
sample_step, nucleus_p = 2, 0.95
beam_width, alpha, decoding_len = 8, 0.6, 128
output = model.diverse_contrastive_search(input_ids, sample_step, nucleus_p, beam_width, alpha, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--sample_step`: The number of tokens sampled with nucleus sampling at the start of generation process.
* `--nucleus_p`: The probability in nucleus sampling.
* `--beam_width`: k in the contrastive search, which is typically set within the range of [0.5,0.8].
* `--alpha`: alpha in the contrastive search, which is typically set within the range of [5,10].
* `--decoding_len`: The total number of tokens to generation. It equals to # of sampled tokens + # of tokens generated by contrastive search.
