### This repo describes the experimental details on Wikitext-103 benchmark.

****
## 目录
* <a href="#Data Preparation">Data Preparation</a>
* [标题](#标题)
* [文本](#文本)
    * 普通文本
    * 单行文本
    * 多行文本
    * 文字高亮
    * 换行
    * 斜体
    * 粗体
    * 删除线
* [图片](#图片)
    * 来源于网络的图片
    * GitHub仓库中的图片
* [链接](#链接) 
    * 文字超链接
        *  链接外部URL
        *  链接本仓库里的URL
    *  锚点
    * [图片链接](#图片链接)
* [列表](#列表)
    * 无序列表
    * 有序列表
    * 复选框列表
* [块引用](#块引用)
* [代码高亮](#代码高亮)
* [表格](#表格) 
* [表情](#表情)
* [diff语法](#diff语法)

#### Data Preparation:
To download the data, please follow the instructions [here](https://github.com/yxuansu/SimCTG/tree/main/data).

> **** The dataset contains the following three files:

    .
    ├── wikitext103                       
        ├── wikitext103_raw_v1_train.txt          # Training Set
        ├── wikitext103_raw_v1_validation.txt     # Validation Set
        └── wikitext103_raw_v1_test.txt           # Test Set

**Data Format**: In the files, each line represents a full document.


#### 2. Train SimCTG:

#### 3. Generate result with different decoding methods:
Here, we use the prefix in Table 3 of the [paper]() to illustrate how to use different decoding methods to generate the result. 
```python
import torch
from simctg import SimCTG
from transformers import AutoTokenizer
# load model and tokenizer
model_path = r'cambridgeltl/simctg_wikitext103'
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = SimCTG(model_path, tokenizer.pad_token_id)
model.eval()

# prepare prefix input
text = r"Butt criticized Donald 's controls in certain situations in the game , as well as the difficulty of some levels and puzzles . Buchanan also criticized the controls , calling"
tokens = tokenizer.tokenize(text)
input_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.LongTensor(input_ids).view(1,-1)
```
##### (1) Contrastive Search:
```python
# use contrastive search to generate the result
beam_width, alpha, decoding_len = 8, 0.6, 128
output = model.fast_contrastive_search(input_ids, beam_width, alpha, decoding_len)
#output = model.slow_contrastive_search(input_ids, beam_width, alpha, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--beam_width`: k in the contrastive search, which is typically set within the range of [0.5,0.8].
* `--alpha`: alpha in the contrastive search, which is typically set within the range of [5,10].
* `--decoding_len`: Number of tokens to generate.

**[Note]** We provide two implementations of contrastive search: (1) fast_contrastive_search and (2) slow_contrastive_search. These two implementations produce the same result, but the fast version is properly optimized and is much faster than the slow version. On the other hand, the implementation details of the slow version is more straightforward. We recommend you to rewrite the slow version first if you want to adapt contrastive search to your specific research task.

##### (2) Diverse Contrastive Search:
We also provide a stochastic version of contrastive search which can generate diverse results by combining nucleus sampling and contrastive search. More details can be found in Appendix E of the [paper]().
```python
# use diverse contrastive search to generate the result
sample_step, nucleus_p = 2, 0.95
beam_width, alpha, decoding_len = 8, 0.6, 128
output = model.diverse_contrastive_search(input_ids, sample_step, nucleus_p, beam_width, alpha, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--sample_step`: The number of tokens sampled with nucleus sampling at the start of generation process.
* `--nucleus_p`: The probability in nucleus sampling.
* `--beam_width`: k in the contrastive search, which is typically set within the range of [0.5,0.8].
* `--alpha`: alpha in the contrastive search, which is typically set within the range of [5,10].
* `--decoding_len`: The total number of tokens to generate. It equals to the number of sampled tokens + the number of tokens generated by contrastive search.

##### (3) Nucleus Sampling:
```python
nucleus_p, decoding_len = 0.95, 128
output = model.nucleus_sampling(input_ids, nucleus_p, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--nucleus_p`: The probability in nucleus sampling.
* `--decoding_len`: Number of tokens to generate.

##### (4) Greedy Search:
```python
decoding_len = 128
output = model.greedy_search(input_ids, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--decoding_len`: Number of tokens to generate.


##### (5) Beam Search:
```python
beam_width, decoding_len = 10, 128
output = model.beam_search(input_ids, beam_width, decoding_len)
print("Output:\n" + 100 * '-')
print(tokenizer.decode(output))
```
The arguments are as follows:
* `--input_ids`: The ids of the prefix sequence.
* `--beam_width`: The beam width of beam search.
* `--decoding_len`: Number of tokens to generate.

